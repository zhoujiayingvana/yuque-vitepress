# hadoop
修改全局环境变量

hadoop-env.sh

core-site.xml

yarn-site.xml

hdfs-site.xml

mapred-site.xml

删除namenode、datanode、tmp文件，重新格式化hdfs

在hdfs里面创建一个spark的日志路径





# hive
替换hadoop的guava到hive里面：hadoop/share/common

上传mysql的jar包

修改全局环境变量

【可选】初始化mysql的元数据、解决中文乱码

把hadoop的aws的jar组件复制到hive的lib里面

hive on spark只能一个beeline用，会造成yarn队列占用问题

# spark
with hadoop版本：确保hadoop和spark里的hadoop版本完全相同。如果不相同，需要替换spark的jar包中的hadoop-common和guava。

without hadoop版本：

1. 复制hive-site.xml到spark的conf下面
2. 在spark-env.sh添加hadoop的classpath
3. 把hive的hive-*的jar包复制到spark的jar文件中。
4. 下载spark-hive.jar和spark-hive-thriftserver.jar，复制到spark的jar包（最后的版本号对应spark的版本号）
5. 把hadoop的guava复制过来

遇到问题：

报错

<font style="color:rgb(31, 35, 40);">NoSuchMethodError: SemaphoredDelegatingExecutor while writing files to S3</font>

替换spark的jar目录下的hadoop-common

报错：

<font style="color:rgb(31, 35, 40);">NoSuchMethodError: org.google.common.base.preconditions.checkargument</font>

<font style="color:rgb(31, 35, 40);">替换spark的jar目录下的guava</font>



# airflow
替换mysql数据库时

数据库连接url改成：mysql+pymysql:// ，不要用mysqldb和mysqlconnector

修改顺序调度器为并发调度

